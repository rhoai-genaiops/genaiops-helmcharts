apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: guidellm-task
spec:
  description: "Run guidellm benchmark against multiple endpoints discovered from canopy-evals repo"
  workspaces:
    - name: shared-workspace
      description: "Shared workspace for storing benchmark results and cloned repo"
  params:
    - name: BASE_URL
      type: string
      description: "Base URL of the target service"
      default: "http://canopy-backend"
    - name: MODEL_NAME
      type: string
      description: "Model name identifier (use canopy_ prefix for Canopy support)"
      default: "canopy_custom"
    - name: PROCESSOR
      type: string
      description: "Processor/model path for tokenization"
      default: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    - name: DATA_CONFIG
      type: string
      description: "Data configuration JSON"
      default: '{"type":"emulated","prompt_tokens":512,"output_tokens":128}'
    - name: RATE_TYPE
      type: string
      description: "Rate type for benchmark"
      default: "synchronous"
    - name: MAX_SECONDS
      type: string
      description: "Maximum benchmark duration in seconds"
      default: "1800"
    - name: MAX_REQUESTS
      type: string
      description: "Maximum number of requests to send"
      default: "5"
    - name: GUIDELLM_IMAGE
      type: string
      description: "Guidellm container image"
      default: "quay.io/rhoai-genaiops/guidellm:v2"
    - name: GIT_SHORT_REVISION
      description: Short Git commit hash for S3 path
      type: string
      default: "latest"
  steps:
    - name: discover-and-benchmark
      image: "$(params.GUIDELLM_IMAGE)"
      workingDir: "$(workspaces.shared-workspace.path)"
      script: |
        #!/bin/bash
        set -e
        
        echo "Discovering test configuration files in cloned repo..."
        
        # Find all *_tests.yaml files (same pattern as KFP pipeline)
        TEST_FILES=$(find . -name "*_tests.yaml" -type f 2>/dev/null || true)
        
        if [ -z "$TEST_FILES" ]; then
          echo "No test configuration files found!"
          echo "Directory contents:"
          ls -la
          exit 1
        fi
        
        echo "Found test files:"
        echo "$TEST_FILES"
        
        # Check if Python is available (should be in guidellm image)
        echo "Checking Python availability..."
        python3 --version || python --version
        
        # Install pyyaml (Python should already be available in guidellm image)
        echo "Installing pyyaml..."
        pip3 install pyyaml || pip install pyyaml
        
        # Create Python script to discover endpoints and run benchmarks
        cat << 'EOF' > discover_and_benchmark.py
        import yaml
        import subprocess
        import sys
        import os
        import glob
        
        def discover_endpoints():
            """Discover unique endpoints from test configuration files"""
            endpoints = set()
            endpoint_files = {}
            
            # Find all test files using glob pattern (same as KFP pipeline)
            test_files = glob.glob("**/*_tests.yaml", recursive=True)
            
            print(f"Processing {len(test_files)} test configuration files...")
            
            for file_path in test_files:
                try:
                    with open(file_path, 'r') as f:
                        config = yaml.safe_load(f)
                    
                    if config and 'endpoint' in config:
                        endpoint = config['endpoint']
                        endpoints.add(endpoint)
                        if endpoint not in endpoint_files:
                            endpoint_files[endpoint] = []
                        endpoint_files[endpoint].append(file_path)
                        print(f"Found endpoint '{endpoint}' in {file_path}")
                    else:
                        print(f"Warning: No endpoint found in {file_path}")
                        
                except Exception as e:
                    print(f"Error reading {file_path}: {e}")
            
            return list(endpoints), endpoint_files
        
        def run_guidellm_for_endpoint(endpoint, base_url, model_name, processor, data_config, rate_type, max_seconds, max_requests):
            """Run guidellm benchmark for a specific endpoint"""
            target_url = f"{base_url}{endpoint}"
            # Create safe filename from endpoint
            safe_endpoint = endpoint.replace('/', '_').replace('-', '_').strip('_')
            output_file = f"guidellm_{safe_endpoint}_results.yaml"
            
            cmd = [
                "guidellm", "benchmark",
                f"--target={target_url}",
                f"--model={model_name}",
                f"--processor={processor}",
                "--backend-type=openai_http",
                f"--data={data_config}",
                f"--output-path={output_file}",
                f"--rate-type={rate_type}",
                f"--max-seconds={max_seconds}",
                f"--max-requests={max_requests}"
            ]
            
            print(f"\nüöÄ Running guidellm benchmark for endpoint: {endpoint}")
            print(f"Target URL: {target_url}")
            print(f"Output file: {output_file}")
            print(f"Command: {' '.join(cmd)}")
            
            try:
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                print(f"‚úÖ Benchmark completed successfully for {endpoint}")
                if result.stdout:
                    print("Stdout:", result.stdout)
                return output_file, True
            except subprocess.CalledProcessError as e:
                print(f"‚ùå Benchmark failed for {endpoint}")
                print(f"Return code: {e.returncode}")
                if e.stdout:
                    print(f"Stdout: {e.stdout}")
                if e.stderr:
                    print(f"Stderr: {e.stderr}")
                return output_file, False
        
        def main():
            # Discover endpoints from test files
            endpoints, endpoint_files = discover_endpoints()
            
            if not endpoints:
                print("‚ùå No endpoints found in test configuration files!")
                return 1
            
            print(f"\nüìã Discovered {len(endpoints)} unique endpoints:")
            for endpoint in sorted(endpoints):
                print(f"  - {endpoint}")
                for file_path in endpoint_files[endpoint]:
                    print(f"    ‚îî‚îÄ‚îÄ {file_path}")
            
            # Get parameters from environment
            base_url = os.environ.get('BASE_URL', 'http://canopy-backend')
            model_name = os.environ.get('MODEL_NAME', 'canopy_custom')
            processor = os.environ.get('PROCESSOR', 'RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8')
            data_config = os.environ.get('DATA_CONFIG', '{"type":"emulated","prompt_tokens":512,"output_tokens":128}')
            rate_type = os.environ.get('RATE_TYPE', 'synchronous')
            max_seconds = os.environ.get('MAX_SECONDS', '1800')
            max_requests = os.environ.get('MAX_REQUESTS', '5')
            
            print(f"\nBenchmark Configuration:")
            print(f"  Base URL: {base_url}")
            print(f"  Model: {model_name}")
            print(f"  Rate Type: {rate_type}")
            print(f"  Max Duration: {max_seconds}s")
            print(f"  Max Requests: {max_requests}")
            
            # Run benchmarks for each discovered endpoint
            successful_benchmarks = []
            failed_benchmarks = []
            
            for i, endpoint in enumerate(sorted(endpoints), 1):
                print(f"\n{'='*50}")
                print(f"Benchmark {i}/{len(endpoints)}: {endpoint}")
                print(f"{'='*50}")
                
                output_file, success = run_guidellm_for_endpoint(
                    endpoint, base_url, model_name, processor, 
                    data_config, rate_type, max_seconds, max_requests
                )
                
                if success:
                    successful_benchmarks.append((endpoint, output_file))
                else:
                    failed_benchmarks.append((endpoint, output_file))
            
            # Write comprehensive summary
            summary_content = []
            summary_content.append("Guidellm Multi-Endpoint Benchmark Summary")
            summary_content.append("=" * 50)
            summary_content.append(f"Total endpoints discovered: {len(endpoints)}")
            summary_content.append(f"Successful benchmarks: {len(successful_benchmarks)}")
            summary_content.append(f"Failed benchmarks: {len(failed_benchmarks)}")
            summary_content.append("")
            
            summary_content.append("Discovered Endpoints:")
            for endpoint in sorted(endpoints):
                summary_content.append(f"  {endpoint}:")
                for file_path in endpoint_files[endpoint]:
                    summary_content.append(f"    - {file_path}")
            summary_content.append("")
            
            if successful_benchmarks:
                summary_content.append("‚úÖ Successful Benchmarks:")
                for endpoint, output_file in successful_benchmarks:
                    summary_content.append(f"  {endpoint} ‚Üí {output_file}")
                summary_content.append("")
            
            if failed_benchmarks:
                summary_content.append("‚ùå Failed Benchmarks:")
                for endpoint, output_file in failed_benchmarks:
                    summary_content.append(f"  {endpoint} ‚Üí {output_file}")
                summary_content.append("")
            
            # Write summary to file
            with open('guidellm_benchmark_summary.txt', 'w') as f:
                f.write('\n'.join(summary_content))
            
            # Print final summary
            print(f"\n{'='*50}")
            print("üìä FINAL BENCHMARK SUMMARY")
            print(f"{'='*50}")
            print('\n'.join(summary_content))
            
            return 0 if len(failed_benchmarks) == 0 else 1
        
        if __name__ == "__main__":
            exit_code = main()
            sys.exit(exit_code)
        EOF
        
        # Run the endpoint discovery and benchmarking
        python3 discover_and_benchmark.py
        
        # Generate HTML reports from YAML files
        echo "üîÑ Converting YAML results to HTML..."
        python3 << 'EOF'
        import os
        import glob
        from guidellm.benchmark import GenerativeBenchmarksReport
        
        # Find all YAML files
        yaml_pattern = "guidellm_*_results.yaml"
        yaml_files = glob.glob(yaml_pattern)
        
        successful_html = []
        failed_html = []
        
        for yaml_file in yaml_files:
            try:
                print(f"Processing {yaml_file}")
                
                # Load the YAML benchmark results
                report = GenerativeBenchmarksReport.load_file(path=yaml_file)
                benchmarks = report.benchmarks
                
                print(f"  üìä Loaded {len(benchmarks)} benchmark(s) from {yaml_file}")
                
                # Extract endpoint from the first benchmark (they should all be the same endpoint)
                if benchmarks and hasattr(benchmarks[0], 'target') and benchmarks[0].target:
                    # Extract endpoint path from target URL (e.g., "http://canopy-backend/summarize" -> "summarize")
                    target_url = str(benchmarks[0].target)
                    if '/' in target_url:
                        endpoint_path = target_url.split('/')[-1]  # Get the last part after final /
                    else:
                        endpoint_path = "unknown"
                    endpoint_name = endpoint_path
                else:
                    # Fallback to filename extraction if no target found
                    endpoint_name = yaml_file.replace("guidellm_", "").replace("_results.yaml", "")
                
                html_filename = f"guidellm_{endpoint_name}_results.html"
                print(f"  Target endpoint: {endpoint_name}")
                print(f"  Generating HTML: {html_filename}")
                
                for benchmark in benchmarks:
                    print(f"    - Benchmark ID: {benchmark.id_}")
                
                # Save as HTML
                report.save_html(path=html_filename)
                print(f"  ‚úÖ Successfully generated HTML report: {html_filename}")
                successful_html.append((yaml_file, html_filename))
                
            except Exception as e:
                print(f"  ‚ùå Error generating HTML report for {yaml_file}: {e}")
                failed_html.append((yaml_file, str(e)))
        
        print(f"\nHTML Generation Summary:")
        print(f"‚úÖ Successful: {len(successful_html)}")
        print(f"‚ùå Failed: {len(failed_html)}")
        
        if failed_html:
            print("Failed conversions:")
            for yaml_file, error in failed_html:
                print(f"  {yaml_file}: {error}")
        EOF
        
        echo "Listing all generated files:"
        ls -la *.yaml *.txt *.html 2>/dev/null || echo "No result files found"
      env:
        - name: BASE_URL
          value: "$(params.BASE_URL)"
        - name: MODEL_NAME
          value: "$(params.MODEL_NAME)"
        - name: PROCESSOR
          value: "$(params.PROCESSOR)"
        - name: DATA_CONFIG
          value: "$(params.DATA_CONFIG)"
        - name: RATE_TYPE
          value: "$(params.RATE_TYPE)"
        - name: MAX_SECONDS
          value: "$(params.MAX_SECONDS)"
        - name: MAX_REQUESTS
          value: "$(params.MAX_REQUESTS)"
    - name: upload-to-s3
      image: "registry.access.redhat.com/ubi9/python-311"
      workingDir: "$(workspaces.shared-workspace.path)"
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_S3_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_S3_ENDPOINT
        - name: AWS_S3_BUCKET
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_S3_BUCKET
        - name: AWS_DEFAULT_REGION
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_DEFAULT_REGION
      script: |
        #!/bin/bash
        set -e
        
        echo "Installing boto3 for S3 upload..."
        python3 -m pip install boto3
        
        echo "Uploading guidellm results to S3..."
        
        cat << 'EOF' | python3
        import os
        import boto3
        from botocore.exceptions import ClientError
        import glob
        import datetime
        
        # Get S3 configuration from environment variables
        s3_endpoint = os.environ.get('AWS_S3_ENDPOINT')
        s3_bucket = os.environ.get('AWS_S3_BUCKET')
        access_key = os.environ.get('AWS_ACCESS_KEY_ID')
        secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
        region = os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')
        git_hash = "$(params.GIT_SHORT_REVISION)"
        model_name = "$(params.MODEL_NAME)"
        
        if not all([s3_endpoint, s3_bucket, access_key, secret_key]):
            print("Warning: S3 configuration incomplete, skipping upload")
            exit(0)
        
        # Initialize S3 client
        s3_client = boto3.client(
            's3',
            endpoint_url=s3_endpoint,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            region_name=region
        )
        
        # Upload all guidellm YAML result files
        yaml_files = glob.glob("guidellm_*_results.yaml")
        uploaded_yamls = []
        failed_yamls = []
        
        if not yaml_files:
            print("Error: No guidellm benchmark YAML files found!")
            print("Looking for pattern: guidellm_*_results.yaml")
            print("Current directory contents:")
            for item in os.listdir('.'):
                print(f"  {item}")
            exit(1)
        
        print(f"Found {len(yaml_files)} YAML result files to upload")
        
        for yaml_file in yaml_files:
            yaml_s3_key = f"{git_hash}/{yaml_file}"
            try:
                print(f"Uploading {yaml_file} to s3://{s3_bucket}/{yaml_s3_key}")
                s3_client.upload_file(yaml_file, s3_bucket, yaml_s3_key)
                print(f"‚úÖ Successfully uploaded {yaml_file}")
                uploaded_yamls.append(yaml_file)
            except ClientError as e:
                print(f"Error uploading {yaml_file}: {e}")
                failed_yamls.append(yaml_file)
        
        # Upload all HTML result files
        html_files = glob.glob("guidellm_*_results.html")
        uploaded_htmls = []
        failed_htmls = []
        
        for html_file in html_files:
            html_s3_key = f"{git_hash}/{html_file}"
            try:
                print(f"Uploading {html_file} to s3://{s3_bucket}/{html_s3_key}")
                s3_client.upload_file(html_file, s3_bucket, html_s3_key)
                print(f"‚úÖ Successfully uploaded {html_file}")
                uploaded_htmls.append(html_file)
            except ClientError as e:
                print(f"Warning: Error uploading {html_file}: {e}")
                failed_htmls.append(html_file)
        
        # Upload the benchmark summary file
        summary_file = "guidellm_benchmark_summary.txt"
        if os.path.exists(summary_file):
            summary_s3_key = f"{git_hash}/{summary_file}"
            try:
                print(f"Uploading {summary_file} to s3://{s3_bucket}/{summary_s3_key}")
                s3_client.upload_file(summary_file, s3_bucket, summary_s3_key)
                print(f"‚úÖ Successfully uploaded {summary_file}")
            except ClientError as e:
                print(f"Warning: Error uploading {summary_file}: {e}")
        
        # Print comprehensive upload summary
        print(f"\n{'='*50}")
        print("üìä S3 UPLOAD SUMMARY")
        print(f"{'='*50}")
        print(f"YAML Files: {len(uploaded_yamls)}/{len(yaml_files)} successful")
        print(f"HTML Files: {len(uploaded_htmls)}/{len(html_files)} successful")
        
        if uploaded_yamls:
            print("\n‚úÖ Successfully uploaded YAML files:")
            for file in uploaded_yamls:
                print(f"  - {file}")
        
        if uploaded_htmls:
            print("\n‚úÖ Successfully uploaded HTML files:")
            for file in uploaded_htmls:
                print(f"  - {file}")
        
        if failed_yamls or failed_htmls:
            print("\n‚ùå Failed uploads:")
            for file in failed_yamls + failed_htmls:
                print(f"  - {file}")
        
        # Exit with error if any YAML files failed to upload (critical)
        if failed_yamls:
            print(f"\nCritical error: {len(failed_yamls)} YAML files failed to upload!")
            exit(1)
        
        print("S3 upload completed successfully!")
        EOF