apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: guidellm-task
spec:
  description: "Run guidellm benchmark against an endpoint and extract results"
  workspaces:
    - name: shared-workspace
      description: "Shared workspace for storing benchmark results"
  params:
    - name: BASE_URL
      type: string
      description: "Base URL of the target service"
      default: "http://canopy-backend"
    - name: ENDPOINT_PATH
      type: string
      description: "Endpoint path to append to base URL"
      default: "/summarize"
    - name: MODEL_NAME
      type: string
      description: "Model name identifier (use canopy_ prefix for Canopy support)"
      default: "canopy_custom"
    - name: PROCESSOR
      type: string
      description: "Processor/model path for tokenization"
      default: "RedHatAI/Llama-3.2-3B-Instruct-quantized.w8a8"
    - name: DATA_CONFIG
      type: string
      description: "Data configuration JSON"
      default: '{"type":"emulated","prompt_tokens":512,"output_tokens":128}'
    - name: OUTPUT_FILENAME
      type: string
      description: "Output filename"
      default: "benchmark-results.yaml"
    - name: RATE_TYPE
      type: string
      description: "Rate type for benchmark"
      default: "synchronous"
    - name: MAX_SECONDS
      type: string
      description: "Maximum benchmark duration in seconds"
      default: "1800"
    - name: MAX_REQUESTS
      type: string
      description: "Maximum number of requests to send"
      default: "5"
    - name: GUIDELLM_IMAGE
      type: string
      description: "Guidellm container image"
      default: "quay.io/rhoai-genaiops/guidellm:v2"
    - name: GIT_SHORT_REVISION
      description: Short Git commit hash for S3 path
      type: string
      default: "latest"
  steps:
    - name: run-benchmark
      image: "$(params.GUIDELLM_IMAGE)"
      command: ["guidellm"]
      args:
        - "benchmark"
        - "--target=$(params.BASE_URL)$(params.ENDPOINT_PATH)"
        - "--model=$(params.MODEL_NAME)"
        - "--processor=$(params.PROCESSOR)"
        - "--backend-type=openai_http"
        - "--data=$(params.DATA_CONFIG)"
        - "--output-path=$(workspaces.shared-workspace.path)/$(params.OUTPUT_FILENAME)"
        - "--rate-type=$(params.RATE_TYPE)"
        - "--max-seconds=$(params.MAX_SECONDS)"
        - "--max-requests=$(params.MAX_REQUESTS)"
    - name: extract-results
      image: "registry.access.redhat.com/ubi9/ubi"
      workingDir: "$(workspaces.shared-workspace.path)"
      script: |
        #!/bin/bash
        set -e
        
        echo "Extracting and organizing benchmark results..."
        
        # Create timestamped directory
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        RESULT_DIR="$(params.MODEL_NAME)_${TIMESTAMP}"
        mkdir -p $RESULT_DIR
        
        # Copy and organize results
        if [ -f "$(params.OUTPUT_FILENAME)" ]; then
          cp "$(params.OUTPUT_FILENAME)" "$RESULT_DIR/"

          # Create summary info
          cat > "$RESULT_DIR/benchmark_info.txt" << EOF
        Model: $(params.MODEL_NAME)
        Target: $(params.BASE_URL)$(params.ENDPOINT_PATH)
        Processor: $(params.PROCESSOR)
        Data Config: $(params.DATA_CONFIG)
        Rate Type: $(params.RATE_TYPE)
        Max Seconds: $(params.MAX_SECONDS)
        Timestamp: $TIMESTAMP
        EOF
          
          # Package results
          tar czf "${RESULT_DIR}.tar.gz" "$RESULT_DIR"
          echo "Results packaged to: ${RESULT_DIR}.tar.gz"
          echo "Contents of workspace:"
          ls -la
        else
          echo "ERROR: Benchmark output file not found: $(params.OUTPUT_FILENAME)"
          exit 1
        fi
    - name: upload-to-s3
      image: "registry.access.redhat.com/ubi9/python-311"
      workingDir: "$(workspaces.shared-workspace.path)"
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_S3_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_S3_ENDPOINT
        - name: AWS_S3_BUCKET
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_S3_BUCKET
        - name: AWS_DEFAULT_REGION
          valueFrom:
            secretKeyRef:
              name: "{{ .Values.secrets.s3.name }}"
              key: AWS_DEFAULT_REGION
      script: |
        #!/bin/bash
        set -e
        
        echo "Installing boto3 for S3 upload..."
        python3 -m pip install boto3
        
        echo "Uploading guidellm results to S3..."
        
        cat << 'EOF' | python3
        import os
        import boto3
        from botocore.exceptions import ClientError
        import glob
        import datetime
        
        # Get S3 configuration from environment variables
        s3_endpoint = os.environ.get('AWS_S3_ENDPOINT')
        s3_bucket = os.environ.get('AWS_S3_BUCKET')
        access_key = os.environ.get('AWS_ACCESS_KEY_ID')
        secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
        region = os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')
        git_hash = "$(params.GIT_SHORT_REVISION)"
        model_name = "$(params.MODEL_NAME)"
        
        if not all([s3_endpoint, s3_bucket, access_key, secret_key]):
            print("Warning: S3 configuration incomplete, skipping upload")
            exit(0)
        
        # Initialize S3 client
        s3_client = boto3.client(
            's3',
            endpoint_url=s3_endpoint,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            region_name=region
        )
        
        # Upload the main benchmark output file
        yaml_file = "$(params.OUTPUT_FILENAME)"
        if not os.path.exists(yaml_file):
            print(f"Error: Benchmark output file not found: {yaml_file}")
            exit(1)
        
        # Upload the raw YAML file
        yaml_s3_key = f"guidellm-benchmarks/{git_hash}/{yaml_file}"
        try:
            print(f"Uploading {yaml_file} to s3://{s3_bucket}/{yaml_s3_key}")
            s3_client.upload_file(yaml_file, s3_bucket, yaml_s3_key)
            print(f"Successfully uploaded {yaml_file}")
        except ClientError as e:
            print(f"Error uploading {yaml_file}: {e}")
            exit(1)
        
        # Upload the packaged tar.gz file (created by extract-results step)
        # The extract-results step creates: {MODEL_NAME}_{TIMESTAMP}.tar.gz
        import glob
        tar_files = glob.glob("*.tar.gz")
        
        for tar_file in tar_files:
            tar_s3_key = f"guidellm-benchmarks/{git_hash}/{tar_file}"
            try:
                print(f"Uploading {tar_file} to s3://{s3_bucket}/{tar_s3_key}")
                s3_client.upload_file(tar_file, s3_bucket, tar_s3_key)
                print(f"Successfully uploaded {tar_file}")
            except ClientError as e:
                print(f"Error uploading {tar_file}: {e}")
                # Don't exit on tar file failure - the main YAML is more important
        
        print("S3 upload completed successfully!")
        EOF