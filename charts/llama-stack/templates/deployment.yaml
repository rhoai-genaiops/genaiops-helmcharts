---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      {{- include "llama-stack.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      creationTimestamp: null
      labels:
        {{- include "llama-stack.selectorLabels" . | nindent 8 }}
    spec:
      volumes:
        - name: run-config-volume
          configMap:
            name: run-config
            defaultMode: 420
        - name: llama-persist
          persistentVolumeClaim:
            claimName: llama-persist
        - name: cache
          emptyDir: {}
        - name: pythain
          emptyDir: {}
      containers:
        - resources: {}
          terminationMessagePath: /dev/termination-log
          name: llama-stack
          env:
            - name: MAX_TOKENS
              value: '128000'
            - name: VLLM_MAX_TOKENS
              value: '128000'
            - name: MODEL_NAME
              value: '{{ .Values.MODEL_NAME }}'
            - name: MODEL_URL
              value: '{{ .Values.MODEL_URL }}'
            - name: VLLM_API_TOKEN
              value: fake
            - name: LLAMA_STACK_LOG
              value: debug
            - name: LLAMA_STACK_PORT
              value: "8321"
            {{- if .Values.otelCollector.enabled }}
            - name: OTEL_SERVICE_NAME
              value: llama-stack
            - name: OTEL_TRACE_ENDPOINT
              value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces
            - name: OTEL_METRIC_ENDPOINT
              value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/metrics
            - name: TELEMETRY_SINKS
              value: "console, sqlite, otel_trace, otel_metric"
            {{- end }}
            {{- if .Values.rag.enabled }}
            - name: MILVUS_DB_PATH
              value: milvus.db
            {{- end }}
          ports:
            - containerPort: 8321
              protocol: TCP
          imagePullPolicy: Always
          volumeMounts:
            - name: pythain
              mountPath: /pythainlp-data
            - name: run-config-volume
              mountPath: /app-config
            - name: llama-persist
              mountPath: /.llama
            - name: cache
              mountPath: /.cache
          terminationMessagePolicy: File
          image: 'docker.io/llamastack/distribution-remote-vllm:latest'
          args:
            - '--config'
            - /app-config/config.yaml

