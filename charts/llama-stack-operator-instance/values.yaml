# Llama Stack Operator Instance configuration
MODEL_NAME: "llama32"
MODEL_URL: "http://llama-32-predictor.ai501.svc.cluster.local:8080/v1"

# Distribution configuration
distribution:
  image: "quay.io/rhoai-genaiops/llama-stack-vllm-milvus-fms:0.2.11"


# ConfigMap for Llama Stack configuration
configMap:
  enabled: true

# Telemetry configuration
telemetry:
  enabled: false

rag:
  enabled: false
  milvus:
    service: "milvus-test"

mcp:
  enabled: false

otelCollector:
  enabled: false

eval:
  enabled: false

guardrails:
  enabled: false
  regex:
    enabled: false
    filter: ["(?i).*fight club.*"]
  hap:
    enabled: false
  prompt_injection:
    enabled: false
  language_detection:
    enabled: false