# Llama Stack Operator Instance configuration
MODEL_NAME: "llama32"
MODEL_URL: "http://llama-32-predictor.ai501.svc.cluster.local:8080/v1"

# Distribution configuration
distribution:
  image: "quay.io/rhoai-genaiops/llama-stack-vllm-milvus:0.2.11"

# ConfigMap for Llama Stack configuration
configMap:
  enabled: true

# Telemetry configuration
telemetry:
  enabled: false

rag:
  enabled: false
  milvus:
    service: "milvus-test"

# Namespace configuration (format: userX-environment)
namespace: "user1-test"

mcp:
  enabled: false

otelCollector:
  enabled: false

eval:
  enabled: false